{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalshavitNYU/michalshavitnyu.github.io/blob/master/CauchyRiemann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRExN8xKdsH4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQo6PaorJM_f",
        "outputId": "3d3d47ed-b990-42fc-c01c-df271c5b61b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.246207\n",
            "Epoch 500, Loss: 0.050505\n",
            "Epoch 1000, Loss: 0.050495\n",
            "Epoch 1500, Loss: 0.050492\n",
            "Epoch 2000, Loss: 0.050491\n",
            "Epoch 2500, Loss: 0.050491\n",
            "Epoch 3000, Loss: 0.050490\n",
            "Epoch 3500, Loss: 0.050491\n",
            "Epoch 4000, Loss: 0.050490\n",
            "Epoch 4500, Loss: 0.050490\n",
            "Epoch 5000, Loss: 0.050490\n",
            "Epoch 5500, Loss: 0.050490\n",
            "Epoch 6000, Loss: 0.050492\n",
            "Epoch 6500, Loss: 0.050490\n",
            "Epoch 7000, Loss: 0.050490\n",
            "Epoch 7500, Loss: 0.050490\n",
            "Epoch 8000, Loss: 0.050490\n",
            "Epoch 8500, Loss: 0.050493\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize a simple fully connected neural network\n",
        "def init_nn_params(layers, key): # A list specifying the number of neurons in each layer and A PRNG key used for reproducible random initialization.\n",
        "    keys = jax.random.split(key, len(layers) - 1) #This splits the PRNG key into separate subkeys, one for each layer-to-layer transition (because we have len(layers) - 1 weight matrices)\n",
        "    params = [] #creates an empty list which we define below in the loop\n",
        "    for k_in, k_out in zip(layers[:-1], layers[1:]): #zip() is a Python function that pairs list: Each layer's input size With the output size of the next layer\n",
        "\n",
        "        w_key, b_key = jax.random.split(keys[0])\n",
        "        params.append(\n",
        "            {\n",
        "                \"W\": jax.random.normal(w_key, (k_in, k_out)) * jnp.sqrt(2.0 / k_in), #He initialization, can change to Xavier without the 2\n",
        "                \"b\": jnp.zeros((k_out,)) #Initial zeroes\n",
        "            }\n",
        "        )\n",
        "        keys = keys[1:] #Moves on to the next subkey for the next layer.\n",
        "    return params\n",
        "\n",
        "# Forward pass through the network #This is the forward pass of the neural network — the part where I feed input through the layers to get the output.\n",
        "def nn_forward(params, xy): #params were created above, xy will be a 2d input vector [x y]\n",
        "    for layer in params[:-1]: #This loops over all layers except the last one — these are the hidden layers.\n",
        "        xy = jnp.tanh(jnp.dot(xy, layer[\"W\"]) + layer[\"b\"])\n",
        "    return jnp.dot(xy, params[-1][\"W\"]) + params[-1][\"b\"]  # Output: (u, v)  this is the final linear transformation that outputs the network's prediction: A 2D vector: [u(x, y), v(x, y)]\n",
        "\n",
        "# Compute partial derivatives correctly #need to improve to vector gradients\n",
        "def compute_derivatives(params, x, y):\n",
        "    \"\"\" Compute derivatives of u and v separately using jax.grad. \"\"\"\n",
        "    def u_func(x, y):\n",
        "        return nn_forward(params, jnp.hstack((x, y)))[0]  # Extract u as a scalar\n",
        "\n",
        "    def v_func(x, y):\n",
        "        return nn_forward(params, jnp.hstack((x, y)))[1]  # Extract v as a scalar\n",
        "\n",
        "    u_x = jax.grad(lambda x: u_func(x, y))(x[0])  # Compute du/dx\n",
        "    u_y = jax.grad(lambda y: u_func(x, y))(y[0])  # Compute du/dy\n",
        "    v_x = jax.grad(lambda x: v_func(x, y))(x[0])  # Compute dv/dx\n",
        "    v_y = jax.grad(lambda y: v_func(x, y))(y[0])  # Compute dv/dy\n",
        "\n",
        "    return u_x, u_y, v_x, v_y\n",
        "\n",
        "# Loss function (Physics-Informed)\n",
        "def loss_fn(params, batch):\n",
        "    x, y = batch[:, 0:1], batch[:, 1:2]\n",
        "\n",
        "    # Compute u, v and their derivatives for each data point individually\n",
        "    u_x, u_y, v_x, v_y = jax.vmap(compute_derivatives, in_axes=(None, 0, 0))(params, x, y)\n",
        "\n",
        "    # Physics loss (Cauchy-Riemann equations)\n",
        "    physics_loss = jnp.mean((u_x - v_y) ** 2 + (u_y + v_x) ** 2)\n",
        "\n",
        "    # Boundary condition at y = 0\n",
        "    x_bc = jnp.linspace(-jnp.pi, jnp.pi, 100).reshape(-1, 1)\n",
        "    xy_bc = jnp.hstack((x_bc, jnp.zeros_like(x_bc)))\n",
        "    uv_bc = nn_forward(params, xy_bc)\n",
        "    boundary_loss = jnp.mean((uv_bc[:, 0] - jnp.cos(x_bc)) ** 2) #+(uv_bc[:, 1] - jnp.sin(x_bc)) ** 2)\n",
        "    #boundary_loss = jnp.mean((uv_bc[:, 0] - jnp.cos(x_bc)) ** 2 + (uv_bc[:, 1] - jnp.sin(x_bc)) ** 2)\n",
        "\n",
        "    return physics_loss + 0.1 * boundary_loss\n",
        "\n",
        "# Generate training data\n",
        "key = jax.random.PRNGKey(42)\n",
        "x_train = jax.random.uniform(key, (1000, 1), minval=-jnp.pi, maxval=jnp.pi)\n",
        "y_train = jax.random.uniform(key, (1000, 1), minval=-jnp.pi, maxval=jnp.pi)\n",
        "train_data = jnp.hstack((x_train, y_train))\n",
        "\n",
        "# Initialize network\n",
        "layers = [2, 64, 64, 64, 2]  # Input: (x,y), Output: (u,v)\n",
        "params = init_nn_params(layers, key)\n",
        "\n",
        "# Define Adam optimizer\n",
        "optimizer = optax.adam(1e-3)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, train_data)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Generate test points for v(0,y)\n",
        "y_test = jnp.linspace(-jnp.pi, jnp.pi, 100).reshape(-1, 1)\n",
        "x_test = jnp.zeros_like(y_test)\n",
        "xy_test = jnp.hstack((x_test, y_test))\n",
        "\n",
        "# Compute v(0,y) using trained network\n",
        "uv_test = nn_forward(params, xy_test)\n",
        "v_test = uv_test[:, 1]  # Extract imaginary part v(x,y)\n",
        "\n",
        "# Plot v(0,y)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(y_test, v_test, label='v(0,y)', color='b')\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('v(0,y)')\n",
        "plt.title('Imaginary Part v(0,y)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpXUM_EBdp_k"
      },
      "outputs": [],
      "source": [
        "# --- Plot v(x, 0) ---\n",
        "x_test_vx0 = jnp.linspace(-jnp.pi, jnp.pi, 100).reshape(-1, 1)\n",
        "y_test_vx0 = jnp.zeros_like(x_test_vx0)\n",
        "xy_test_vx0 = jnp.hstack((x_test_vx0, y_test_vx0))\n",
        "uv_test_vx0 = nn_forward(params, xy_test_vx0)\n",
        "v_x0 = uv_test_vx0[:, 1]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x_test_vx0, v_x0, label='v(x, 0)', color='g')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('v(x,0)')\n",
        "plt.title('Imaginary Part v(x, 0)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCdwUWWpdqvb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Check Harmonicity: Laplacians Δu and Δv ----\n",
        "def laplacians(params, xy):\n",
        "    def u_func(xy): return nn_forward(params, xy)[0]\n",
        "    def v_func(xy): return nn_forward(params, xy)[1]\n",
        "\n",
        "    hess_u = jax.jacfwd(jax.grad(u_func))(xy)\n",
        "    hess_v = jax.jacfwd(jax.grad(v_func))(xy)\n",
        "    delta_u = jnp.trace(hess_u)\n",
        "    delta_v = jnp.trace(hess_v)\n",
        "    return delta_u, delta_v\n",
        "\n",
        "# Sample points on a grid\n",
        "grid_x, grid_y = jnp.meshgrid(jnp.linspace(-jnp.pi, jnp.pi, 100),\n",
        "                              jnp.linspace(-jnp.pi, jnp.pi, 100))\n",
        "xy_grid = jnp.stack([grid_x.ravel(), grid_y.ravel()], axis=-1)\n",
        "lap_u, lap_v = jax.vmap(lambda xy: laplacians(params, xy))(xy_grid)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.contourf(grid_x, grid_y, lap_u.reshape(100, 100), levels=50)\n",
        "plt.colorbar()\n",
        "plt.title(\"Laplacian Δu\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.contourf(grid_x, grid_y, lap_v.reshape(100, 100), levels=50)\n",
        "plt.colorbar()\n",
        "plt.title(\"Laplacian Δv\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- 2D Field Plots of u(x,y) and v(x,y) ----\n",
        "uv_vals = jax.vmap(lambda xy: nn_forward(params, xy))(xy_grid)\n",
        "u_vals = uv_vals[:, 0].reshape(100, 100)\n",
        "v_vals = uv_vals[:, 1].reshape(100, 100)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.contourf(grid_x, grid_y, u_vals, levels=50)\n",
        "plt.colorbar()\n",
        "plt.title(\"u(x, y)\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.contourf(grid_x, grid_y, v_vals, levels=50)\n",
        "plt.colorbar()\n",
        "plt.title(\"v(x, y)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XLaoNGZMTZ0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQOxVXwekjzybc03sS0LZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}